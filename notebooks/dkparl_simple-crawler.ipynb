{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search url for resumes\n",
    "\n",
    "SEARCH_URL_FORMAT = \"https://www.ft.dk/da/dokumenter/dokumentlister/referater?startDate={}&endDate={}&pageSize=200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set dates\n",
    "\n",
    "startdate = \"20200101\" ## format YYYYMMDD\n",
    "enddate = \"20220130\" ## format YYYYMMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set search string\n",
    "\n",
    "search_url = SEARCH_URL_FORMAT.format(startdate, enddate)\n",
    "PAGE_URL_FORMAT = search_url + \"&pageNumber={}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302resultater\n"
     ]
    }
   ],
   "source": [
    "## Results\n",
    "\n",
    "r = requests.get(search_url)\n",
    "soup = bs(r.text, \"html.parser\")\n",
    "\n",
    "print(soup.find(\"span\", class_ = \"results\").get_text(strip = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of pages\n",
    "\n",
    "no_pages = soup.find(\"ul\", class_ = \"pagination pagination-centered text-center\").find_all(\"li\")[-1].get_text()\n",
    "\n",
    "if no_pages is None:\n",
    "    more_pages = False\n",
    "else:\n",
    "    more_pages = True\n",
    "    no_pages = int(no_pages)\n",
    "    \n",
    "if more_pages:\n",
    "    pagenumbers = list(range(2, no_pages+1)) # Excuding first page as it corresponds to search_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get resume links\n",
    "\n",
    "resume_links = [link.find('a')['href'] for link in soup.find_all(\"td\", attrs = {\"data-title\": \"Mødedato, -tid og samling\"})]\n",
    "\n",
    "if more_pages:\n",
    "    for pagenumber in pagenumbers:\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        page_url = PAGE_URL_FORMAT.format(str(pagenumber))\n",
    "        r = requests.get(page_url)\n",
    "        soup = bs(r.text, \"html.parser\")\n",
    "        \n",
    "        page_links = [link.find('a')['href'] for link in soup.find_all(\"td\", attrs = {\"data-title\": \"Mødedato, -tid og samling\"})]\n",
    "        \n",
    "        resume_links = resume_links + page_links\n",
    "        \n",
    "resume_links = [urljoin(search_url, resume_link) for resume_link in resume_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resume_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ft.dk/forhandlinger/20211/20211M053_2022-01-28_1000.htm'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume scraper\n",
    "\n",
    "def resume_scraper(url):\n",
    "    \n",
    "    agendaregex = re.compile('Dagsorden.*')\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    soup = bs(r.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"p\", class_ = \"Titel\").get_text()\n",
    "    except AttributeError:\n",
    "        title = \"\"\n",
    "        \n",
    "    try:\n",
    "        subtitle = soup.find(\"p\", class_ = \"UnderTitel\").get_text()\n",
    "    except AttributeError:\n",
    "        subtitle = \"\"\n",
    "    \n",
    "    try:\n",
    "        agenda = '\\n'.join([tag.get_text() for tag in soup.find_all(\"p\", class_ = agendaregex)])\n",
    "    except AttributeError:\n",
    "        agenda = \"\"    \n",
    "    \n",
    "    \n",
    "    resume_dict = {}\n",
    "    resume_dict['url'] = url\n",
    "    resume_dict['title'] = title\n",
    "    resume_dict['subtitle'] = subtitle\n",
    "    resume_dict['agenda'] = agenda\n",
    "    resume_dict['text'] = soup.get_text()\n",
    "    \n",
    "    return(resume_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|==================================================| 100.00 %\r"
     ]
    }
   ],
   "source": [
    "## Scrape all sites\n",
    "\n",
    "resumes = []\n",
    "\n",
    "for c, resume_link in enumerate(resume_links, start = 1):\n",
    "    resume = resume_scraper(resume_link)\n",
    "    \n",
    "    resumes.append(resume)\n",
    "    \n",
    "    progress = \"|{0}| {1:.2f} %\".format((\"=\"*int(c/len(resume_links) * 50)).ljust(50), c/len(resume_links) * 100)\n",
    "    print(progress, end = \"\\r\")\n",
    "    \n",
    "    time.sleep(random.uniform(0.5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_f = os.path.join(\"..\", \"data\")\n",
    "out_n = \"testdata_20220210.json\"\n",
    "out_p = os.path.join(out_f, out_n)\n",
    "\n",
    "with open(out_p, \"w\") as f:\n",
    "    json.dump(resumes, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
